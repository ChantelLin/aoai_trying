{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44842274-5c07-4589-aabf-76cba6eaa42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install if needed\n",
    "# %pip install pypdf\n",
    "# %pip install pandas\n",
    "# %pip install -qU langchain-community \n",
    "# %pip install --upgrade --quiet azure-search-documents\n",
    "# %pip install --upgrade --quiet azure-identity\n",
    "# %pip install langchain_openai\n",
    "# %pip install -U azure-search-documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28baf96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.retrievers import AzureCognitiveSearchRetriever\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "import os\n",
    "\n",
    "# seems useless\n",
    "os.environ[\"AZURESEARCH_FIELDS_ID\"] = \"chunk_id\"\n",
    "os.environ[\"AZURESEARCH_FIELDS_CONTENT\"] = \"chunk\"\n",
    "os.environ[\"AZURESEARCH_FIELDS_CONTENT_VECTOR\"] = \"text_vector\"\n",
    "os.environ[\"AZURESEARCH_FIELDS_TAG\"] = \"metadata_storage_path\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa76da4",
   "metadata": {},
   "source": [
    "##### Creating Index :: Azure Cognitive Search (Vector Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "# get config from aoai_config.yaml\n",
    "\n",
    "%run env_config_setting.ipynb\n",
    "\n",
    "openai_api_version = openai.api_version\n",
    "openai_api_key = openai.api_key\n",
    "openai_api_endpoint = openai.api_base\n",
    "\n",
    "print('openi config: ', deployment_name, embedding_name, openai_api_key, openai_api_endpoint, openai_api_version)\n",
    "print('search config: ', search_service_name, search_key, search_endpoint)\n",
    "print('storageaccnt: ', storageaccnt)\n",
    "\n",
    "index_name = \"azure-rag-test-index\"\n",
    "\n",
    "# Search Index Schema definition\n",
    "index_schema = \"./openai-search-demo.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a82e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import CorsOptions\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SearchIndex \n",
    "\n",
    "## Class to create index ::\n",
    "\n",
    "# Instantiate a client\n",
    "class CreateClient(object):\n",
    "    def __init__(self, endpoint, key, index_name):\n",
    "        self.endpoint = endpoint\n",
    "        self.index_name = index_name\n",
    "        self.key = key\n",
    "        self.credentials = AzureKeyCredential(key)\n",
    "\n",
    "    # Create a SearchClient\n",
    "    # Use this to upload docs to the Index\n",
    "    def create_search_client(self):\n",
    "        return SearchClient(\n",
    "            endpoint=self.endpoint,\n",
    "            index_name=self.index_name,\n",
    "            credential=self.credentials,\n",
    "        )\n",
    "\n",
    "    # Create a SearchIndexClient\n",
    "    # This is used to create, manage, and delete an index\n",
    "    def create_admin_client(self):\n",
    "        return SearchIndexClient(endpoint=self.endpoint, credential=self.credentials)\n",
    "\n",
    "\n",
    "# Get Schema from File or URL\n",
    "def get_schema_data(schema, url=False):\n",
    "    if not url:\n",
    "        with open(schema) as json_file:\n",
    "            schema_data = json.load(json_file)\n",
    "            return schema_data\n",
    "    else:\n",
    "        data_from_url = requests.get(schema)\n",
    "        schema_data = json.loads(data_from_url.content)\n",
    "        return schema_data\n",
    "\n",
    "# Convert CSV data to JSON\n",
    "def convert_csv_to_json(url):\n",
    "    df = pd.read_csv(url)\n",
    "    convert = df.to_json(orient=\"records\")\n",
    "    return json.loads(convert)\n",
    "\n",
    "# Create Search Index from the schema\n",
    "# If reading the schema from a URL, set url=True\n",
    "def create_schema_from_json_and_upload(schema, index_name, admin_client, url=False):\n",
    "\n",
    "    cors_options = CorsOptions(allowed_origins=[\"*\"], max_age_in_seconds=60)\n",
    "    scoring_profiles = []\n",
    "    schema_data = get_schema_data(schema, url)\n",
    "    print(\"schema_data:\", schema_data)\n",
    "    # the schema_data does not define field 'content_vector'\n",
    "\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=schema_data[\"fields\"],\n",
    "        scoring_profiles=scoring_profiles,\n",
    "        suggesters=schema_data[\"suggesters\"],\n",
    "        cors_options=cors_options,\n",
    "    )\n",
    "    print(\"index:\", index)\n",
    "    \n",
    "    try:\n",
    "        print(\"call admin_client.create_index..\")\n",
    "        upload_schema = admin_client.create_index(index)\n",
    "        if upload_schema:\n",
    "            print(f\"Schema uploaded; Index created for {index_name}.\")\n",
    "        else:\n",
    "            exit(0)\n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exception()) #sys.exc_info()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0f6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "start_client = CreateClient(search_endpoint, search_key, index_name)\n",
    "\n",
    "admin_client = start_client.create_admin_client()\n",
    "search_client = start_client.create_search_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7c692e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'openai-search-demo', 'fields': [{'name': 'id', 'type': 'Edm.String', 'searchable': True, 'filterable': True, 'retrievable': True, 'stored': True, 'sortable': True, 'facetable': True, 'key': True, 'indexAnalyzer': None, 'searchAnalyzer': None, 'analyzer': 'keyword', 'normalizer': None, 'dimensions': None, 'vectorSearchProfile': None, 'vectorEncoding': None, 'synonymMaps': []}, {'name': 'content', 'type': 'Edm.String', 'facetable': False, 'filterable': False, 'key': False, 'retrievable': True, 'searchable': True, 'sortable': False, 'analyzer': 'standard.lucene', 'indexAnalyzer': None, 'searchAnalyzer': None, 'synonymMaps': [], 'fields': []}, {'name': 'metadata', 'type': 'Edm.String', 'searchable': False, 'filterable': False, 'retrievable': True, 'stored': True, 'sortable': False, 'facetable': False, 'key': False, 'indexAnalyzer': None, 'searchAnalyzer': None, 'analyzer': None, 'normalizer': None, 'dimensions': None, 'vectorSearchProfile': None, 'vectorEncoding': None, 'synonymMaps': []}, {'name': 'category', 'type': 'Edm.String', 'facetable': False, 'filterable': True, 'key': False, 'retrievable': True, 'searchable': True, 'sortable': False, 'analyzer': 'standard.lucene', 'indexAnalyzer': None, 'searchAnalyzer': None, 'synonymMaps': [], 'fields': []}], 'suggesters': [], 'scoringProfiles': [], 'defaultScoringProfile': '', 'corsOptions': {'allowedOrigins': ['*'], 'maxAgeInSeconds': 300}, 'analyzers': [], 'charFilters': [], 'tokenFilters': [], 'tokenizers': [], 'encryptionKey': None, 'similarity': {'@odata.type': '#Microsoft.Azure.Search.BM25Similarity', 'k1': None, 'b': None}}\n"
     ]
    }
   ],
   "source": [
    "# check schema data\n",
    "schema_data = get_schema_data(index_schema, url=False)\n",
    "print(schema_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171ef5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'allowed_origins': ['*'], 'max_age_in_seconds': 60}\n"
     ]
    }
   ],
   "source": [
    "# check scoring profiles\n",
    "scoring_profiles = []\n",
    "cors_options = CorsOptions(allowed_origins=[\"*\"], max_age_in_seconds=60)\n",
    "print(cors_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "025220da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema_data: {'name': 'openai-search-demo', 'fields': [{'name': 'id', 'type': 'Edm.String', 'searchable': True, 'filterable': True, 'retrievable': True, 'stored': True, 'sortable': True, 'facetable': True, 'key': True, 'indexAnalyzer': None, 'searchAnalyzer': None, 'analyzer': 'keyword', 'normalizer': None, 'dimensions': None, 'vectorSearchProfile': None, 'vectorEncoding': None, 'synonymMaps': []}, {'name': 'content', 'type': 'Edm.String', 'facetable': False, 'filterable': False, 'key': False, 'retrievable': True, 'searchable': True, 'sortable': False, 'analyzer': 'standard.lucene', 'indexAnalyzer': None, 'searchAnalyzer': None, 'synonymMaps': [], 'fields': []}, {'name': 'metadata', 'type': 'Edm.String', 'searchable': False, 'filterable': False, 'retrievable': True, 'stored': True, 'sortable': False, 'facetable': False, 'key': False, 'indexAnalyzer': None, 'searchAnalyzer': None, 'analyzer': None, 'normalizer': None, 'dimensions': None, 'vectorSearchProfile': None, 'vectorEncoding': None, 'synonymMaps': []}, {'name': 'category', 'type': 'Edm.String', 'facetable': False, 'filterable': True, 'key': False, 'retrievable': True, 'searchable': True, 'sortable': False, 'analyzer': 'standard.lucene', 'indexAnalyzer': None, 'searchAnalyzer': None, 'synonymMaps': [], 'fields': []}], 'suggesters': [], 'scoringProfiles': [], 'defaultScoringProfile': '', 'corsOptions': {'allowedOrigins': ['*'], 'maxAgeInSeconds': 300}, 'analyzers': [], 'charFilters': [], 'tokenFilters': [], 'tokenizers': [], 'encryptionKey': None, 'similarity': {'@odata.type': '#Microsoft.Azure.Search.BM25Similarity', 'k1': None, 'b': None}}\n",
      "index: {'additional_properties': {}, 'name': 'azure-rag-test-index', 'fields': [{'name': 'id', 'type': 'Edm.String', 'searchable': True, 'filterable': True, 'retrievable': True, 'stored': True, 'sortable': True, 'facetable': True, 'key': True, 'indexAnalyzer': None, 'searchAnalyzer': None, 'analyzer': 'keyword', 'normalizer': None, 'dimensions': None, 'vectorSearchProfile': None, 'vectorEncoding': None, 'synonymMaps': []}, {'name': 'content', 'type': 'Edm.String', 'facetable': False, 'filterable': False, 'key': False, 'retrievable': True, 'searchable': True, 'sortable': False, 'analyzer': 'standard.lucene', 'indexAnalyzer': None, 'searchAnalyzer': None, 'synonymMaps': [], 'fields': []}, {'name': 'metadata', 'type': 'Edm.String', 'searchable': False, 'filterable': False, 'retrievable': True, 'stored': True, 'sortable': False, 'facetable': False, 'key': False, 'indexAnalyzer': None, 'searchAnalyzer': None, 'analyzer': None, 'normalizer': None, 'dimensions': None, 'vectorSearchProfile': None, 'vectorEncoding': None, 'synonymMaps': []}, {'name': 'category', 'type': 'Edm.String', 'facetable': False, 'filterable': True, 'key': False, 'retrievable': True, 'searchable': True, 'sortable': False, 'analyzer': 'standard.lucene', 'indexAnalyzer': None, 'searchAnalyzer': None, 'synonymMaps': [], 'fields': []}], 'scoring_profiles': [], 'default_scoring_profile': None, 'cors_options': <azure.search.documents.indexes._generated.models._models_py3.CorsOptions object at 0x138b02e10>, 'suggesters': [], 'analyzers': None, 'tokenizers': None, 'token_filters': None, 'char_filters': None, 'encryption_key': None, 'similarity': None, 'semantic_search': None, 'vector_search': None, 'e_tag': None}\n",
      "call admin_client.create_index..\n",
      "Unexpected error: (ResourceNameAlreadyInUse) Cannot create index 'azure-rag-test-index' because it already exists.\n",
      "Code: ResourceNameAlreadyInUse\n",
      "Message: Cannot create index 'azure-rag-test-index' because it already exists.\n",
      "Exception Details:\t(CannotCreateExistingIndex) Cannot create index 'azure-rag-test-index' because it already exists.\n",
      "\tCode: CannotCreateExistingIndex\n",
      "\tMessage: Cannot create index 'azure-rag-test-index' because it already exists.\n"
     ]
    }
   ],
   "source": [
    "# index openai-search-demo successfully created in s100-ai-search | Indexes\n",
    "# if index already was created, skip this block !!\n",
    "\n",
    "index = create_schema_from_json_and_upload(index_schema, index_name, admin_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f4e0249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "pdf_folder_path = './pdfs/'\n",
    "loaders = [PyPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]\n",
    "documents = []\n",
    "for loader in tqdm(loaders):\n",
    "    try:\n",
    "        documents.extend(loader.load())\n",
    "    except:\n",
    "        pass\n",
    "with open('my_documents.pkl', 'wb') as f:\n",
    "    pickle.dump(documents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb589c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e4e0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_size=1000\n",
    "# embeddings: OpenAIEmbeddings = OpenAIEmbeddings(model=model,deployment=deployment,\n",
    "#                                    openai_api_base=openai.api_base,\n",
    "#                                   openai_api_type = \"azure\",\n",
    "#                                   chunk_size=1)\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=embedding_name,\n",
    "    openai_api_version=openai_api_version,\n",
    "    azure_endpoint=openai_api_endpoint,\n",
    "    api_key=openai_api_key,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c8afa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {\"source\": \"./pdfs/eda_llm_future.pdf\", \"page\": 0}\n",
      "1 {\"source\": \"./pdfs/eda_llm_future.pdf\", \"page\": 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare data to fill in index\n",
    "# need manually add filed content_vector\n",
    "\n",
    "from operator import itemgetter\n",
    "batch_array = []\n",
    "\n",
    "for i,content in enumerate(texts):\n",
    "    metad_str = str(content.metadata).replace('\\'','\"')\n",
    "    print(i, metad_str)\n",
    "    batch_array.append(\n",
    "        {\n",
    "            \"id\": str(i),\n",
    "            \"content\": content.page_content,\n",
    "            \"metadata\": metad_str, #json format like this  \"metadata\": \"{\\\"source\\\": \\\"./pdfs/eda_llm_future.pdf\\\", \\\"page\\\": 1}\",\n",
    "            \"category\": \"CATEGORY\",\n",
    "            \"content_vector\": embeddings.embed_query(content.page_content) \n",
    "        })\n",
    "len(batch_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7e276fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': '0', 'succeeded': True, 'status_code': 200}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload data \n",
    "\n",
    "results = search_client.upload_documents(documents=batch_array)\n",
    "results[0].as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e58b582a-566d-4462-aaf1-4e4fa9c85c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48009038 Large Language Models for EDA: Future or Mirage?\n",
      "Z\n",
      "0.27359986 design automation, such as decoupled hardware temp\n"
     ]
    }
   ],
   "source": [
    "# RAG search\n",
    "\n",
    "result2 = search_client.search(search_text=\"circuit\")\n",
    "# print(result2)\n",
    "for rs in result2:\n",
    "    print(rs['@search.score'], rs['content'][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f47a7e39-2fb0-4f43-8e63-5702c6717849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "# langchain AzureSearch 會預設parse field content_vetor, metadata\n",
    "# 如果沒有這兩個欄位 或資料格式不對 就會回error\n",
    "\n",
    "# 即使上面 AZURESEARCH_FIELDS_CONTENT_VECTOR 改成 text_vector\n",
    "# parse 的時候還是拿到 content_vector\n",
    "\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=search_endpoint,\n",
    "    azure_search_key=search_key,\n",
    "    index_name= index_name, # use other index because above index does not have content_vector\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    # Configure max retries for the Azure client\n",
    "    additional_search_client_options={\"retry_total\": 4},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7510cb-2e14-4c3a-aeb6-f7516b4cedee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a similarity search\n",
    "\n",
    "docs = vector_store.similarity_search(\n",
    "    query=\"circuit\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "\n",
    "# print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ede2bffa-654f-459d-86cf-78e605c7412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'id': '0', 'source': './pdfs/eda_llm_future.pdf', 'page': 0} Large Language Models for EDA: Future or Mirage?\n",
      "Z\n",
      "{'id': '1', 'source': './pdfs/eda_llm_future.pdf', 'page': 1} design automation, such as decoupled hardware temp\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "for doc in docs:\n",
    "    print(doc.metadata, doc.page_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6976200",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ad55117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is chateda?',\n",
       " 'context': [Document(metadata={'id': '1', 'source': './pdfs/eda_llm_future.pdf', 'page': 1}, page_content='design automation, such as decoupled hardware template design, pri-\\noritizing in-context learning given limited data, and proper prompt\\nengineering. ChipChat [ 4] authors argue that when human feed-\\nback is incorporated into the more advanced ChatGPT-4 model or\\nused for co-design, the language model acts as a ’force multiplier’,\\nenabling quick exploration and iteration of the design space.\\nThe second interesting direction is to help the design flow with\\nLLM. ChatEDA [ 8] addresses the challenge of integrating EDA tools\\nto improve interoperability in circuit design, by leveraging LLM\\ncapabilities in natural language processing and comprehension. An\\nautonomous agent for EDA that utilizes the AutoMage LLM in con-\\njunction with EDA tools as executors is proposed. It offers users\\na conversational interface to interact with these tools. Users can\\ncommunicate their requirements using natural language prompts,\\nand ChatEDA’s goal is to generate executable programs (scripts) that\\nfulfill the user’s specific needs. Acting as the controller, ChatEDA\\norchestrates the collaboration between these tools. It starts by cre-\\nating a task list based on user requirements and then generates\\nscripts for each task. The conversational interaction may inspire\\nnext-generation EDA tool evolution.\\nThe third direction is debugging in Verilog: RTLFixer [ 9] is an au-\\ntomated framework that addresses syntax-related errors in Verilog\\ncode using LLMs. It consists of an LLM for code generation, RAG for\\naccessing expert guidance, and ReAct for task decomposition and\\nplanning. The framework starts by formulating an input prompt\\ncombining a benchmark problem with a template. RAG and ReAct\\nare then used to revise the code and resolve errors. Persistent syntax\\nerrors are addressed with feedback from compiler error logs and\\nretrieved human guidance. This iterative debugging process contin-\\nues until all errors are resolved. RTLFixer achieves a high success\\nrate of 98 .5%in fixing compilation errors and significantly improves\\npass rates in VerilogEval benchmarks. Specifically, it enhances the\\npass@1 success rates by 32 .3%in VerilogEval-Machine and 10 .1%in\\nVerilogEval-Human benchmarks.\\n3 CONCLUSION AND FUTURE DIRECTIONS\\nThis paper has embarked on a comprehensive exploration of the\\nintegration of LLMs into EDA. Our analysis reveals a landscape\\nbrimming with potential, yet punctuated by significant challenges\\nand uncertainties. The promise of LLMs in enhancing EDA processes\\nis undeniable, offering novel approaches to design automation, error\\ndetection, and efficiency improvements. However, the path to fully\\nrealizing this potential is fraught with technical, ethical, and practical\\nobstacles.\\nThe future of LLMs in EDA hinges on several key factors. Firstly,\\nthe development of more specialized LLMs tailored to understand\\nthe intricacies of electronic design languages and processes is crucial.\\nThe current generation of LLMs, while advanced, is predominantly\\ntrained on general language data. For LLMs to be truly transforma-\\ntive in EDA, they must evolve to comprehend the specialized syntax\\nand semantics of electronic design, a process that requires exten-\\nsive domain-specific data and training. Data generation techniques\\nlike self-instruct [ 10] are of great significance. Careful selection or\\ncombination between RAG and fine-tuning [ 11] is also crucial in\\nachieving optimal results.Secondly, addressing ethical and practical concerns is paramount.\\nIssues such as data security, privacy, and the ethical use of AI in\\nprofessional settings need rigorous standards and regulatory frame-\\nworks. Recent studies have demonstrate the vulnerabilities of LLMs\\nwhen faced with malicious user interactions, and prompt injection\\ntechniques are proposed [ 12]. As LLMs become more integrated into\\nEDA, it is vital to ensure that these tools are used responsibly, with\\na clear understanding of their limitations and potential biases.\\nLooking ahead, the future directions of LLM integration into\\nEDA are manifold. One promising avenue is the development of\\nAI-assisted design tools that seamlessly integrate with existing EDA\\nsoftware, offering enhanced capabilities in design optimization and\\nerror correction. Another area of exploration is the use of LLMs\\nin educational settings, where they can aid in training the next\\ngeneration of electronic designers, providing interactive learning\\nexperiences and personalized feedback.\\nFurthermore, there is substantial scope for collaborative research\\nbetween AI experts and EDA professionals. Such collaboration can\\naccelerate the development of customized LLMs that cater specifi-\\ncally to the needs of the EDA industry. Additionally, exploring the\\npotential of LLMs in related fields like chip layout optimization\\nand predictive maintenance of electronic systems could open new\\nfrontiers in the application of AI in electronics.\\nIn conclusion, while the integration of LLMs into EDA presents a\\nlandscape rich with opportunities, it remains a complex and evolv-\\ning field. Continued research, interdisciplinary collaboration, and\\nmindful consideration of ethical implications are essential in steer-\\ning this integration towards a future that is not only technologically\\nadvanced but also responsible and inclusive. As we stand at the\\ncusp of this technological evolution, it is imperative to navigate this\\njourney with a balanced perspective, embracing innovation while\\nremaining cognizant of the challenges that lie ahead.\\nREFERENCES\\n[1]M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney, R. Liang, J. Alben, H. Anand,\\nS. Banerjee, I. Bayraktaroglu et al., “ChipNeMo: Domain-Adapted LLMs for Chip\\nDesign, ” arXiv preprint arXiv:2311.00176, 2023.\\n[2]L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,\\nS. Agarwal, K. Slama, A. Ray et al., “Training language models to follow instructions\\nwith human feedback, ” Proc. NeurIPS, vol. 35, pp. 27 730–27 744, 2022.\\n[3]S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan, R. Karri, B. Dolan-Gavitt, and\\nS. Garg, “Benchmarking Large Language Models for Automated Verilog RTL Code\\nGeneration, ” in Proc. DATE, 2023, pp. 1–6.\\n[4]J. Blocklove, S. Garg, R. Karri, and H. Pearce, “Chip-Chat: Challenges and Oppor-\\ntunities in Conversational Hardware Design, ” in Proc. MLCAD, 2023.\\n[5]M. Liu, N. Pinckney, B. Khailany, and H. Ren, “VerilogEval: Evaluating Large\\nLanguage Models for Verilog Code Generation, ” in Proc. ICCAD, 2023.\\n[6]Y. Fu, Y. Zhang, Z. Yu, S. Li, Z. Ye, C. Li, C. Wan, and Y. C. Lin, “GPT4AIGChip:\\nTowards next-generation AI accelerator design automation via large language\\nmodels, ” in Proc. ICCAD, 2023.\\n[7]K. Chang, Y. Wang, H. Ren, M. Wang, S. Liang, Y. Han, H. Li, and X. Li,\\n“ChipGPT: How far are we from natural language hardware design, ” arXiv preprint\\narXiv:2305.14019, 2023.\\n[8]Z. He, H. Wu, X. Zhang, X. Yao, S. Zheng, H. Zheng, and B. Yu, “ChatEDA: A Large\\nLanguage Model Powered Autonomous Agent for EDA, ” in Proc. MLCAD, 2023.\\n[9]Y. Tsai, M. Liu, and H. Ren, “RTLFixer: Automatically Fixing RTL Syntax Errors\\nwith Large Language Models, ” arXiv preprint arXiv:2311.16543, 2023.\\n[10] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi,\\n“Self-instruct: Aligning language model with self generated instructions, ” arXiv\\npreprint arXiv:2212.10560, 2022.\\n[11] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang,\\n“Retrieval-augmented generation for large language models: A survey, ” arXiv\\npreprint arXiv:2312.10997, 2023.\\n[12] F. Perez and I. Ribeiro, “Ignore previous prompt: Attack techniques for language\\nmodels, ” arXiv preprint arXiv:2211.09527, 2022.\\nISPD ’24, March 12–15, 2024, Taipei, Taiwan\\nZhuolun He & Bei Yu\\n66'),\n",
       "  Document(metadata={'id': '0', 'source': './pdfs/eda_llm_future.pdf', 'page': 0}, page_content='Large Language Models for EDA: Future or Mirage?\\nZhuolun He\\nThe Chinese University of Hong Kong\\nHong Kong SARBei Yu\\nThe Chinese University of Hong Kong\\nHong Kong SAR\\nABSTRACT\\nIn this paper, we explore the burgeoning intersection of Large Lan-\\nguage Models (LLMs) and Electronic Design Automation (EDA). We\\ncritically assess whether LLMs represent a transformative future\\nfor EDA or merely a fleeting mirage. By analyzing current advance-\\nments, challenges, and potential applications, we dissect how LLMs\\ncan revolutionize EDA processes like design, verification, and op-\\ntimization. Furthermore, we contemplate the ethical implications\\nand feasibility of integrating these models into EDA workflows. Ulti-\\nmately, this paper aims to provide a comprehensive, evidence-based\\nperspective on the role of LLMs in shaping the future of EDA.\\nACM Reference Format:\\nZhuolun He and Bei Yu. 2024. Large Language Models for EDA: Future or\\nMirage? . In Proceedings of the 2024 International Symposium on Physical\\nDesign (ISPD ’24), March 12–15, 2024, Taipei, Taiwan. ACM, New York, NY,\\nUSA, 2pages. https://doi.org/10.1145/3626184.3639700\\n1 INTRODUCTION\\nThe advent of Large Language Models (LLMs) has ushered in a\\nnew era in the realm of artificial intelligence, redefining the bound-\\naries of machine learning and its applications. One such field that\\nstands on the cusp of potential transformation is Electronic De-\\nsign Automation (EDA). EDA, a cornerstone in the semiconductor\\nindustry, encompasses a suite of software tools for designing elec-\\ntronic systems such as integrated circuits and printed circuit boards.\\nThe integration of LLMs into EDA promises to revolutionize this\\nfield, offering unprecedented capabilities in design automation, error\\ndetection, and process optimization.\\nThis paper seeks to explore the intersection of LLMs with EDA,\\ncritically examining whether this integration heralds a new future\\nor is merely an overhyped concept. The surge in LLM capabilities,\\ncharacterized by models such as GPT-4, has demonstrated profound\\npotential in understanding and generating human-like text. This\\ncapability, when applied to EDA, could offer transformative solutions\\nin automating complex design processes, providing natural language\\ninterfaces for design tools, and enhancing the accuracy of predictive\\nmodels used in circuit design and analysis.\\nHowever, the marriage of LLMs with EDA is not without its chal-\\nlenges. The complexity of EDA tasks, coupled with the specialized\\nnature of electronic design languages, poses a significant hurdle.\\nMoreover, the integration of LLMs raises pertinent ethical and prac-\\ntical concerns. Issues such as data privacy, model reliability, and the\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nISPD ’24, March 12–15, 2024, Taipei, Taiwan\\n©2024 Copyright held by the owner/author(s).\\nACM ISBN 979-8-4007-0417-8/24/03.\\nhttps://doi.org/10.1145/3626184.3639700potential for automation-induced obsolescence in skilled professions\\nare critical considerations that must be addressed.\\nIn this paper, we delve into the nuances of applying LLMs to EDA.\\nWe analyze current trends, potential applications, and the challenges\\nfaced in actualizing this integration. By examining case studies and\\nemerging research, we aim to provide a comprehensive overview of\\nhow LLMs could potentially reshape the future of EDA, evaluating\\nwhether this integration is a groundbreaking advancement or an\\noverestimated prospect.\\n2 APPLICATIONS FOR EDA\\nThere are many applications applying LLM into circuit design and\\nthe EDA flow. For example, ChipNeMo [ 1] aims to optimize the\\nutilization of LLMs in chip design by employing domain adaptation\\ntechniques instead of relying solely on off-the-shelf LLMs. These\\ntechniques include custom tokenizers, domain-adaptive continued\\npretraining, supervised fine-tuning with domain-specific instruc-\\ntions, and domain-adapted retrieval models. The effectiveness of\\nthese techniques is evaluated through three selected LLM applica-\\ntions: an engineering assistant chatbot, EDA script generation, and\\nbug summarization and analysis. By training on 128 A100 GPUs,\\nChipNeMo has demonstrated impressive results in the selected ap-\\nplications. However, the evaluation results indicate a considerable\\ndisparity compared to human expert performance. To bridge this\\nperformance gap, several approaches are being considered by the au-\\nthors, including data collection with more internal proprietary data,\\nintegrating better code-based base model, conducting reinforcement\\nlearning from human feedback (RLHF) [ 2], and investigating better\\nRAG methods.\\nThe most popular application is to help circuit design with LLM:\\nprior works [ 3–7], among many others, are all very recent attempts\\ntowards this direction. VerilogEval [ 5] introduces a benchmarking\\nframework specifically designed to evaluate the performance of\\nLLMs in generating Verilog code for hardware design and verifica-\\ntion. The evaluation dataset covers a wide range of Verilog code\\ngeneration tasks, varying from simple combinational circuits to com-\\nplex finite state machines. To assess the functional correctness of the\\ngenerated Verilog code, transient simulation outputs are compared\\nwith a golden solution, enabling automated testing. Similarly, Thakur\\net al. [ 3] fine-tune pre-trained LLMs using Verilog datasets sourced\\nfrom GitHub and Verilog textbooks. They develop an evaluation\\nframework that includes test-benches for functional analysis and a\\nflow to test the syntax of the generated Verilog code across various\\nproblem scenarios. GPT4AIGChip [ 6] is a framework that leverages\\nLLMs and human natural languages to democratize AI accelera-\\ntor design. The framework features an automated demo-augmented\\nprompt-generation pipeline that utilizes in-context learning to guide\\nLLMs in generating high-quality AI accelerator designs. The authors\\nhighlight the insights and guidelines derived from understanding the\\nlimitations and exploitable capabilities of LLMs for AI accelerator\\n65')],\n",
       " 'answer': 'ChatEDA is an autonomous agent for Electronic Design Automation (EDA) that leverages Large Language Models (LLMs) to improve interoperability in circuit design. It provides a conversational interface for users to interact with EDA tools, allowing them to communicate their requirements using natural language prompts. ChatEDA generates executable programs (scripts) to fulfill user-specific needs and orchestrates the collaboration between various EDA tools.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "client = AzureChatOpenAI(\n",
    "    azure_deployment=deployment_name,  # or your deployment\n",
    "    api_version=openai_api_version,  # or your api version\n",
    "    temperature=0,\n",
    "    api_key=openai_api_key,\n",
    "    azure_endpoint=openai_api_endpoint)\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(client, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "query = \"What is chateda?\"\n",
    "chain.invoke({\"input\": query})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
